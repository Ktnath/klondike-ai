# =======================
# ğŸŒ ENVIRONNEMENT
# =======================
env:
  name: KlondikeEnv
  observation_dim: 156         # Taille du vecteur d'observation encodÃ©
  action_dim: 96               # Nombre total dâ€™actions possibles
  use_intentions: true         # Active la fusion intention + observation
  reward_function: base        # base | critical | heuristic
  seed: 42

# =======================
# ğŸ§  MODELE
# =======================
model:
  type: "dqn"                  # Type de modÃ¨le utilisÃ© (dqn, dueling_dqn, etc.)
  pretrained_path: "model.pt"  # Chemin vers le modÃ¨le prÃ©-entraÃ®nÃ©
  hidden_dims: [256, 256]
  dueling: true                # Active Dueling DQN
  double: true                 # Active Double DQN
  dropout: 0.1                 # Pour Ã©viter lâ€™overfitting
  use_intention_embedding: true

# =======================
# ğŸ‹ï¸â€â™‚ï¸ ENTRAINEMENT DQN
# =======================
training:
  batch_size: 64
  buffer_size: 100000
  learning_rate: 0.00025
  gamma: 0.99
  tau: 0.005                  # Soft update des cibles
  update_every: 4
  train_start: 1000
  max_steps: 500000
  target_update_freq: 1000
  episodes: 10000
  epsilon:
    start: 1.0
    min: 0.05
    decay: 0.995

# =======================
# ğŸ¯ DATASETS
# =======================
dataset:
  expert_data: "data/expert_dataset.npz"
  replay_data: "data/self_play_replays.npz"
  test_data: "data/test_dataset.npz"
  format: npz                 # npz | csv

expert_dataset: "data/expert_dataset.npz"  # Chemin vers le dataset expert
dagger_dataset: "data/dagger_buffer.jsonl" # Chemin vers le buffer DAgger
imitation_learning: false                   # Active l'apprentissage par imitation
dagger: false                               # Active l'entraÃ®nement DAgger
critical_weighting: false                   # Active la pondÃ©ration des mouvements critiques

# =======================
# ğŸ” SELF-PLAY
# =======================
self_play:
  enabled: true
  num_games: 1000
  num_workers: 4
  sampling_strategy: epsilon-greedy
  save_path: "data/self_play_replays.npz"
  merge_strategy: append     # append | replace
  fine_tune_epochs: 5

# =======================
# ğŸ” EVALUATION
# =======================
evaluation:
  episodes: 100
  eval_every: 10000
  render: false
  model_path: "checkpoints/model_final.pth"

# =======================
# ğŸ“š INTENTIONS
# =======================
intentions:
  enabled: true
  mode: one-hot              # one-hot | embedding | hierarchical
  filter_noise: true
  simplify: true
  hierarchy:
    - reveal
    - foundation
    - stack_move
    - king_to_empty

# =======================
# ğŸ§© INTENTION EMBEDDING
# =======================
intention_embedding:
  type: onehot             # onehot | embedding | hierarchical
  dimension: 4             # nombre de dimensions si embedding
  combine_mode: concat     # concat | add | multiply
  dropout: 0.1             # rÃ©gularisation optionnelle
  use_hierarchy: false     # activer la hiÃ©rarchisation (si applicable)
  # List of fine-grained intentions to remove before training
  filter_list: []
  # Mapping of specific intentions to more generic ones
  replacements: {}

# =======================
# ğŸ“ LOGGING & CHECKPOINTS
# =======================
logging:
  log_dir: logs
  save_model_every: 10000
  checkpoint_dir: checkpoints
  tensorboard: true
  verbose: true
  log_interval: 100                         # FrÃ©quence de log
  save_interval: 1000                       # FrÃ©quence de sauvegarde
  log_path: "results/train_log.csv"         # Fichier de log
  enable_logging: true                      # Active/dÃ©sactive les logs CSV
